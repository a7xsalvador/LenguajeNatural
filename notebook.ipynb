{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Fases de entrenamiento de una red neuronal\n\nImportamos los datos de un corpus en español.\n\nCorpus de español:\n- AnCora | Github: https://github.com/UniversalDependencies/UD_Spanish-AnCora\n\n- usamos el conllu parser para leer el corpus: https://pypi.org/project/conllu/ . Es muy usado, se puede trabajar con pip install conllu\n\n- Etiquetas Universal POS (Documentación): https://universaldependencies.org/u/pos/ . Hay referencias universales para clasificar palabras\n",
      "metadata": {
        "tags": [],
        "cell_id": "00002-9a0d49c7-2434-4f8e-a4b9-72fb4a5b829e",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00003-0e79af9c-7208-44a0-ad09-cc42d949687c",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "5c94efc5",
        "execution_millis": 4380,
        "execution_start": 1616448949608,
        "deepnote_cell_type": "code"
      },
      "source": "!pip install conllu\n!git clone https://github.com/UniversalDependencies/UD_Spanish-AnCora.git",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting conllu\n  Downloading conllu-4.4-py2.py3-none-any.whl (15 kB)\nInstalling collected packages: conllu\nSuccessfully installed conllu-4.4\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.0.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nfatal: destination path 'UD_Spanish-AnCora' already exists and is not an empty directory.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00002-15eee690-5152-4216-bc67-3ba09a20716d",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "cd228da6",
        "execution_millis": 7,
        "execution_start": 1616448955575,
        "deepnote_cell_type": "code"
      },
      "source": "#leer los datos que vienen de un archivo conllu\nfrom conllu import parse_incr\nwordlist = []#creo una lista vacía\ndata_file = open(\"UD_Spanish-AnCora/es_ancora-ud-dev.conllu\", \"r\", encoding=\"utf-8\") #abro el archivo que descargué al clonar el repositorio, agrego permisos de lectura con \"r\"\n\n#for tokenlist in parse_incr(data_file):\n#    print(tokenlist.serialize())\n    \n    #quiero ver los tokens\n    #serialize es un atributo propio del formato conllu, sirve para poder ver bien los tokens\n",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Veamos la estructura de los tokens",
      "metadata": {
        "tags": [],
        "cell_id": "00005-86563dac-3109-4beb-9f95-a92d326d1ce6",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00004-feff9758-0bd2-4b0b-a68b-cdaa1d90a974",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "d0d4b2ed",
        "execution_millis": 59,
        "execution_start": 1616086186697,
        "deepnote_cell_type": "code"
      },
      "source": "print(tokenlist)\nprint(tokenlist[1])",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "TokenList<Lo, cierto, es, que, a, mí, ,, me, da, un, poco, de, pena, .>\ncierto\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00005-2f90f574-153c-4a82-9e83-5e920b1d592f",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "6e9610cc",
        "execution_millis": 40,
        "execution_start": 1616086186717,
        "deepnote_cell_type": "code"
      },
      "source": "tokenlist[1]['form']+'|'+tokenlist[1]['upos']# el atributo form es una palabra, upos es la categoría gramatical.",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "'cierto|ADJ'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00006-2e9e8893-ca11-43fe-bab1-8731ac6d71c0",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "64fb3f4e",
        "execution_millis": 48,
        "execution_start": 1616086186717,
        "deepnote_cell_type": "code"
      },
      "source": "for i in range(10):\n    print(tokenlist[i]['form']+'|'+tokenlist[i]['upos'])",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Lo|PRON\ncierto|ADJ\nes|AUX\nque|SCONJ\na|ADP\nmí|PRON\n,|PUNCT\nme|PRON\nda|VERB\nun|DET\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "# Entrenamiento del modelo- Calculo de conteos\nLa primera etapa es el cálculo de conteos\n\n- tags(tags) ``tagCountDict``: $C(tag)$\n- emisiones(word|tag) ``emissionProbDict``: $C(word|tag)$\n- transiciones(tag|prevtag) ``transitionDict``: $C(tag|prevtag)$",
      "metadata": {
        "tags": [],
        "cell_id": "00007-a6000be8-4cdc-4655-8ea6-e44be6801683",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00008-ce5e9451-b8af-4481-94ba-3e6be8c537f8",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "8891f856",
        "execution_millis": 1153,
        "execution_start": 1616086186771,
        "deepnote_cell_type": "code"
      },
      "source": "#tenemos tres diccionarios inicialmente vacíos\n\ntagCountDict = {}\nemissionDict = {}\ntransitionDict = {}\n\ntagtype = 'upos'\ndata_file = open(\"UD_Spanish-AnCora/es_ancora-ud-dev.conllu\", \"r\", encoding=\"utf-8\") #abro el archivo que descargué al clonar el repositorio, agrego permisos de lectura con \"r\"\nfor tokenlist in parse_incr(data_file):\n    prevtag=None\n    for token in tokenlist:\n        #C(tag) el conteo de las etiquetas\n        tag = token[tagtype]\n        if tag in tagCountDict.keys():\n            tagCountDict[tag] +=1\n        else:\n            tagCountDict[tag] = 1\n\n        #C(WORD|tag) -> probabilidades de emisión\n        wordtag = token['form'].lower()+'|'+token[tagtype] #(palabra|etiqueta)\n        if wordtag in emissionDict.keys():\n            emissionDict[wordtag] +=1\n        else:\n            emissionDict[wordtag] = 1\n\n        #Ahora vamos a hacer un conteo para las probabilidades de transición\n        #C(tag|tag_previo)\n        if prevtag is None:\n            prevtag=tag\n            continue\n        transitiontags = tag+'|'+prevtag\n        if transitiontags in transitionDict.keys():\n            transitionDict[transitiontags] = transitionDict[transitiontags] + 1\n        else:\n            transitionDict[transitiontags] = 1\n        prevtag = tag    \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Cálculo de probabilidades\n\n- Probabilidades de transición\n$$\nP(tag|prevtag)=\\frac{C(prevtag,tag)}{C(prevtag)}\n$$\n\n\n- Probabilidades de emisión\n\n$$\nP(word|tag)=\\frac{C(word,tag)}{C(tag)}\n$$",
      "metadata": {
        "tags": [],
        "cell_id": "00009-4fcfe40c-0efe-47e9-a39b-52b1a7c25086",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00010-91d40dc5-407e-45ec-90fc-8b6ad40d9ca6",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "6f803984",
        "execution_millis": 2,
        "execution_start": 1616086187938,
        "deepnote_cell_type": "code"
      },
      "source": "transitionProbDict = {} # matriz A\nemissionProbDict = {} # matriz B\n\n# transition Probabilities \nfor key in transitionDict.keys():\n    tag, prevtag = key.split('|')\n    if tagCountDict[prevtag]>0:\n        transitionProbDict[key] = transitionDict[key]/(tagCountDict[prevtag])\n    else:\n        print(key)\n\n# emission Probabilities \nfor key in emissionDict.keys():\n    word, tag = key.split('|')\n    if emissionDict[key]>0:\n        emissionProbDict[key] = emissionDict[key]/tagCountDict[tag]\n    else:\n        print(key)\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00012-a87c9c41-28db-4b33-9a21-10db3652eb2d",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "bd6d16e2",
        "execution_millis": 17,
        "execution_start": 1616086187943,
        "deepnote_cell_type": "code"
      },
      "source": "#Ya tenemos el modelo, ahora tenemos que guardarlo\nimport numpy as np\nnp.save('transitionHMM.npy', transitionProbDict)\nnp.save('emissionHMM.npy',  emissionProbDict)\n\n#si queremos volver a cargar el modelo ya entrenado\ntransitionProbDict = np.load('transitionHMM.npy', allow_pickle='TRUE').item()\ntransitionProbDict['ADJ|ADJ']",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "0.030225988700564973"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "# Algoritmo de Viterbi\n\nSe trata de hallar la probabilidad de viterbi $v_t(j)$ en cada elemento $j$ de una cadena de palabras.\n\n$$\nv_t(j) = \\max_i{(v_{t-1}(i) \\times C_{ij}\\times P(palabra|j))}\n$$\n\nPara el primer elemento, la probabilidad de Viterbi está dada por\n$$\nv_1(j)= \\rho_j^{(0)} \\times P(palabra|j)\n$$\n\ndonde $\\rho_j^{(0)}$ es la probabilidad de encontrar la categoría gramatical $j$.\n",
      "metadata": {
        "tags": [],
        "cell_id": "00013-c71d324a-488f-4c86-b841-51a73df57e2d",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00013-5893ab0d-790a-4b4c-82d1-c61ecccdf931",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "67ad11fd",
        "execution_millis": 2,
        "execution_start": 1616086187964,
        "deepnote_cell_type": "code"
      },
      "source": "#identificamos las categorías gramaticales 'upos' únicas en el corpus\n\nstateSet = set([w.split('|')[1] for w in list(emissionProbDict.keys())])",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00014-d16d942f-1000-4ecb-bf8c-784b8c38c1af",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "c4c1bf7d",
        "execution_millis": 49,
        "execution_start": 1616086187970,
        "deepnote_cell_type": "code"
      },
      "source": "#A cada categoría asignamos un índice entero \n\ntagStateDict = {}\nfor i, state in enumerate(stateSet):\n    tagStateDict[state] = i\n\ntagStateDict",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00015-46d3a433-c680-4098-96b8-d5bedfe65493",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "4c558bcb",
        "execution_millis": 1059,
        "execution_start": 1616086188019,
        "deepnote_cell_type": "code"
      },
      "source": "#Calculamos distribución inicial de estados\nwordlist = []#creo una lista vacía\ndata_file = open(\"UD_Spanish-AnCora/es_ancora-ud-dev.conllu\", \"r\", encoding=\"utf-8\") #abro el archivo que descargué al clonar el repositorio, agrego permisos de lectura con \"r\"\n\ncount = 0 # contador de la longitud del corpus\n\ninitTagStateProb = {} #\\rho_i^0\nfor tokenlist in parse_incr(data_file):\n    count += 1\n    tag = tokenlist[0]['upos']\n    if tag in initTagStateProb.keys():\n        initTagStateProb[tag] +=1\n    else:\n        initTagStateProb[tag]=1\n\nfor key in initTagStateProb.keys():\n    initTagStateProb[key] /= count\n\ninitTagStateProb\n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "{'DET': 0.3633615477629988,\n 'PROPN': 0.1124546553808948,\n 'ADP': 0.16384522370012092,\n 'PRON': 0.034461910519951636,\n 'SCONJ': 0.02418379685610641,\n 'ADV': 0.06287787182587666,\n 'PUNCT': 0.07799274486094317,\n 'VERB': 0.04353083434099154,\n 'ADJ': 0.010882708585247884,\n 'CCONJ': 0.03325272067714631,\n 'NOUN': 0.02720677146311971,\n '_': 0.0006045949214026602,\n 'INTJ': 0.0006045949214026602,\n 'AUX': 0.022370012091898428,\n 'NUM': 0.01995163240628779,\n 'SYM': 0.0006045949214026602,\n 'PART': 0.0018137847642079807}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00016-dbf160a2-cb0c-4d48-94fc-5e4335618505",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "6cd41a9e",
        "execution_millis": 17,
        "execution_start": 1616086189064,
        "deepnote_cell_type": "code"
      },
      "source": "#Verificamos si la suma de las probabilidades nos da uno\n\nnp.array([initTagStateProb[k] for k in  initTagStateProb.keys()]).sum()",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "1.0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "# Construcción del algoritmo de Viterbi",
      "metadata": {
        "tags": [],
        "cell_id": "00017-0fb3d18a-45ca-492c-b31c-60de30ae99c0",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00018-cc25b111-1e44-4754-adcc-6fb7451b4f15",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "483da7d1",
        "execution_millis": 1525,
        "execution_start": 1616086189073,
        "deepnote_cell_type": "code"
      },
      "source": "import nltk\nnltk.download('punkt')\nfrom nltk import word_tokenize\n",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00019-84043c50-a457-4844-832f-49c4980bd29c",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "22b4ec83",
        "execution_millis": 28,
        "execution_start": 1616086190607,
        "deepnote_cell_type": "code"
      },
      "source": "def Viterbitags(secuencia, transitionProbDict = transitionProbDict, emissionProbDict = emissionProbDict, tagStateDict= tagStateDict, initTagStateProb=initTagStateProb):\n    #inicialización de la primera columna\n\n    seq = word_tokenize(secuencia)\n    viterbiProb = np.zeros((17, len(seq)))\n\n    # para la primera columna\n    for key in tagStateDict.keys():\n        tag_row = tagStateDict[key]\n        word_tag= seq[0].lower()+'|'+key\n        if word_tag in emissionProbDict.keys():\n            viterbiProb[tag_row, 0 ] = initTagStateProb[key]*emissionProbDict[word_tag]\n\n    # para las siguientes columnas\n    for col in range(1, len(seq)):\n        for key in tagStateDict.keys():\n            tag_row = tagStateDict[key]\n            word_tag= seq[col].lower()+'|'+key\n            if word_tag in emissionProbDict.keys():\n                possible_probs = []\n                for key2 in tagStateDict.keys():\n                    tag_row2 = tagStateDict[key2]\n                    tag_prevtag = key+'|'+key2\n                    if tag_prevtag in transitionProbDict.keys():\n                        if viterbiProb[tag_row2, col-1]>0:\n                            possible_probs.append(\n                                viterbiProb[tag_row2, col-1]*transitionProbDict[tag_prevtag]*emissionProbDict[word_tag])\n                viterbiProb[tag_row, col] = max(possible_probs)\n\n    #construccion de la sección de tags\n    res= []\n    for i, p in enumerate(seq):\n        for tag in tagStateDict.keys():\n            if tagStateDict[tag] == np.argmax(viterbiProb[:, i]):\n                res.append((p,tag))\n\n    return res\n\nmatrix = Viterbitags('el mundo es pequeño')\nmatrix\n\n\n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "[('el', 'DET'), ('mundo', 'NOUN'), ('es', 'AUX'), ('pequeño', 'ADJ')]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "# Entrenamiento directo con NLTK\n",
      "metadata": {
        "tags": [],
        "cell_id": "00021-77a4b82a-e152-4fc0-bead-a10862386fea",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00021-578882d8-d85a-4846-9058-650823fd4685",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "d1ef6246",
        "execution_millis": 1295,
        "execution_start": 1616086190631,
        "deepnote_cell_type": "code"
      },
      "source": "import nltk\nnltk.download('treebank')\nfrom nltk.corpus import treebank\ntrain_data = treebank.tagged_sents()[:3900]  #este dataset ya está tageado\n\n",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "[nltk_data] Downloading package treebank to /root/nltk_data...\n[nltk_data]   Unzipping corpora/treebank.zip.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00022-021c3fa5-a108-473a-97b3-e8c68814cb3b",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "dc669d3a",
        "execution_millis": 9,
        "execution_start": 1616086191931,
        "deepnote_cell_type": "code"
      },
      "source": "train_data",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00023-1473046b-ace8-437d-a10b-7ebd5f652246",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "802f82c8",
        "execution_millis": 1409,
        "execution_start": 1616086191938,
        "deepnote_cell_type": "code"
      },
      "source": "# Vamos a usr directamente el algoritmo de nltk para entrenar\n\nfrom nltk.tag import hmm\ntagger = hmm.HiddenMarkovModelTrainer().train_supervised(train_data)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00024-7bdb7e06-fb2a-41e7-ad0d-30aa08363d57",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "bab0118a",
        "execution_millis": 856,
        "execution_start": 1616086193377,
        "deepnote_cell_type": "code"
      },
      "source": "tagger.tag(\"Pierre Vinken will get old\".split())",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "[('Pierre', 'NNP'),\n ('Vinken', 'NNP'),\n ('will', 'MD'),\n ('get', 'VB'),\n ('old', 'JJ')]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00025-bf7ee1c0-969c-4faa-bfbb-51471b7a0342",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "aca5b5a1",
        "execution_millis": 28168,
        "execution_start": 1616086194236,
        "deepnote_cell_type": "code"
      },
      "source": "tagger.evaluate(train_data)",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "0.9815403947224078"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=31a57299-7db0-417b-9a80-6e17fdb92497' target=\"_blank\">\n<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote_notebook_id": "44c9718d-246d-47ba-ab69-7530ee19dcda",
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_execution_queue": []
  }
}